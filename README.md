# BehavioralTwin: Q-Former Based Personalization of LLMs

> **Beyond PPlug**: We replace simple weighted averaging with a **Q-Former cross-attention module** to better model user behavior for LLM personalization.

## üîç –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç

–ü—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç **BehavioralTwin** ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ LLM, –∫–æ—Ç–æ—Ä–∞—è:
- –ö–æ–¥–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ **–∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–π BGE —ç–Ω–∫–æ–¥–µ—Ä**
- –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ **Q-Former** (learnable queries + –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ)
- –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å –≤ **Flan-T5-XXL / Qwen** –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å–∞–º–æ–π LLM

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: **+5.05% accuracy** –Ω–∞ LaMP-1 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å PPlug (0.8997 vs 0.8492).

## üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
[User History] ‚Üí Behavioral Encoder ‚Üí [P√ó768]
                                  ‚Üò
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ   Q-FORMER    ‚îÇ ‚Üê 8 –æ–±—É—á–∞–µ–º—ã—Ö queries
                                    ‚îÇ ‚Ä¢ Cross-att (history) ‚îÇ
                                    ‚îÇ ‚Ä¢ Cross-att (input)   ‚îÇ
                                    ‚îÇ ‚Ä¢ Self-att (queries)  ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üí [Q√óH]
                                  ‚Üó
[Current Query] ‚Üí Input Encoder  ‚Üí [1√ó768]

[Q√óH] ‚Üí Proj ‚Üí Prefix ‚Üí LLM ‚Üí [1] or [2]
```

### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
- **`BehavioralEncoder`**: –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–π BGE –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏
- **`SimpleTextEncoder`**: —á–∞—Å—Ç–∏—á–Ω–æ –æ–±—É—á–∞–µ–º—ã–π BGE –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (`tune_layers=4`)
- **`QFormer`**: —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (–Ω–∞—à–∞ –Ω–æ–≤–∏–∑–Ω–∞!)
- **`FusionModel`**: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Flan-T5 (encoder-decoder) –∏ Qwen (causal)

## ‚öôÔ∏è –ó–∞–ø—É—Å–∫

```bash
accelerate launch --config_file config/ds_config.yaml train.py
```

### –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã (`train.py`)
```python
mode = "sequential"  # –æ–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ –ø–æ –∑–∞–¥–∞—á–∞–º (LaMP-1, LaMP-2, ...)
mode = "joint"       # –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
mode = "eval_only"   # —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫

# –î–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è:
resume_from = "saved/checkpoints/your_model.pt"

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (LaMP-1):
lr = 1e-4
warmup_ratio = 0.05
num_queries = 8
batch_size = 4  # –¥–ª—è Flan-T5-XXL
```

## üìä –î–∞–Ω–Ω—ã–µ

- **–û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç**: [LaMP Benchmark](https://lamp-benchmark.github.io/download)
- **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: [Research Papers Dataset (Kaggle)](https://www.kaggle.com/datasets/nechbamohammed/research-papers-dataset)

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∞

```
‚îú‚îÄ‚îÄ model.py             # BehavioralTwin, QFormer, FusionModel
‚îú‚îÄ‚îÄ pplug.py             # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è baseline (PPlug)
‚îú‚îÄ‚îÄ dataset/             # LaMPDataset —Å author-disjoint split
‚îú‚îÄ‚îÄ trainers/            # TaskSequentialTrainer, ModelEvaluator
‚îî‚îÄ‚îÄ train.py             # –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
```

## üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–†–∞–∑–Ω–∏—Ü–∞ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ PPlug:

| –ú–µ—Ç—Ä–∏–∫–∞ | –†–∞–∑–Ω–∏—Ü–∞ (Abs. Delta) |
|----------|-----------------------|
| Accuracy | +5.046%               |
| Precision| +5.026%               |
| Recall   | +5.051%               |


‚Üí –£–ª—É—á—à–µ–Ω–∏–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –∑–∞ —Å—á—ë—Ç **–±–æ–ª–µ–µ –±–æ–≥–∞—Ç–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ Q-Former**.
