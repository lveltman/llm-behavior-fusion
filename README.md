# Behavioral LLM Fusion

### Что делает проект

Этот проект обучает модель, которая **объединяет поведенческие признаки и текстовые данные** для решения различных NLP-задач (LaMP benchmark).
Мы используем архитектуру из трёх компонентов:

* **Behavioral Encoder** — кодировщик поведенческих признаков (например, BGE-base-en).
* **QFormer** — модуль, преобразующий эмбеддинги признаков в формат, совместимый с LLM.
* **FusionModel (Flan-T5)** — крупная языковая модель, которая принимает как текст, так и поведенческий префикс.

Модель обучается в двух режимах:

* **Sequential** — задачи обрабатываются по очереди, метрики замеряются после каждой.
* **Joint** — обучение сразу на объединённых данных.
* **Eval only** — пропускаем обучение, сразу измеряем метрики для готовой модели.

### Особенности

* **Заморозка LLM и fine-tuning только QFormer/prefix\_proj** для экономии памяти.

### Запуск

```bash
screen -S train -L -Logfile train.log accelerate launch \
  --config_file config/ds_config.yaml train.py
```

В `main.py` доступны режимы:

```python
mode = "joint"       # обучение на всех задачах сразу
mode = "sequential"  # обучение по задачам
mode = "eval_only"   # только замер метрик без обучения
```

Для продолжения обучения с чекпоинта:

```python
resume_from = "saved/checkpoints/model_joint_YYYYMMDD_HHMMSS.pt"
```

### Структура кода

* **`BehavioralEncoder`** — кодировщик поведенческих данных.
* **`QFormer`** — модуль трансформации эмбеддингов.
* **`FusionModel`** — T5 с поддержкой префиксных эмбеддингов.
* **`JointTaskTrainer`** / **`TaskSequentialTrainer`** — логика обучения.
* **`ModelEvaluator`** — единая точка для измерения метрик.
* **`ModelSaver`** — сохранение моделей в нужном формате.

### Что можно улучшить

* Разморозить несколько слоёв LLM или encoder.
* Подключить LoRA.
* Попробовать разные LR для разных компонентов: \~1e-3 для prefix\_proj, \~1e-4 для QFormer.
* Добавить cosine scheduler с warmup.
